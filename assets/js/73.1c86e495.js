(window.webpackJsonp=window.webpackJsonp||[]).push([[73],{1258:function(t,a,s){"use strict";s.r(a);var n=s(2),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("div",{staticClass:"custom-block tip"},[s("p",[t._v("这里记录微软开发的Qlib框架学习, 主要结合2021年发表在AAAI上的文献一起学习.")])]),t._v(" "),s("blockquote",[s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("task type")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("'train' or 'eval'")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[t._v("log_dir")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("e.g., 'OUTPUT_DIR' +  'example/PPO'")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[t._v("buffer_size")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("e.g., 80000, the size of replay buffer")])])])])]),t._v(" "),s("h2",{attrs:{id:"backtest-generation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#backtest-generation"}},[t._v("#")]),t._v(" BackTest Generation")]),t._v(" "),s("h2",{attrs:{id:"order-generation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#order-generation"}},[t._v("#")]),t._v(" Order Generation")]),t._v(" "),s("h2",{attrs:{id:"io"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#io"}},[t._v("#")]),t._v(" IO")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Sampler and Loggers")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("train_paths")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("raw_dir")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" ../data/backtest/\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("order_dir")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" ../data/order/train/\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("io_conf")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("test_sampler")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" TestSampler\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("train_sampler")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Sampler\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("test_logger")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" DFLogger\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("env_conf")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("features")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" raw\n          "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" range\n          "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("loc")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" ../data/normed_feature/\n          "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("size")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("180")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" teacher_action\n          "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" interval\n          "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("size")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("loc")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" ../data/feature/teacher/\n")])])]),s("Tabs",{attrs:{type:"",card:"undefined"}},[s("Tab",{attrs:{label:"Sampler"}},[s("p",[t._v("Random Sampling of Instrument ID during the Training")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train_sampler "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("getattr")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sampler"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" io_conf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train_sampler"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_paths"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" env_conf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'features'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nself"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("env"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sampler "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train_sampler\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Sampler")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        attr$       raw_dir, order_dir, ins_list, features, queue, child, ins, raw_df, df_list, order_df\n        ins_list$   300 target securities in my example, CSI 300 \n        date$       2021-01-04 ~ 2021-01-18 in my case, 11 days \n        feature$    2020-09-15 ~ 2021-01-18 2880 features each day and each security\n        """')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("_worker")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("order_dir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" raw_dir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" features"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ins_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" queue"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        read    order    @order_dir\n                feature  @feature[\'loc\'], might from multiple location\n                backtest @raw_dir\n            \n        day_raw_df = get backtest information (instrument and date)\n                            $close0     $vwap0      $volume0\n        datetime        \n        2021-01-04 09:30:00 0.972727    0.972054    2.153626e+07\n        2021-01-04 09:31:00 0.969697    0.971212    8.579835e+06\n        \n        day_feature_dfs = read feature from multiple resource\n                                FEATURE_0   FEATURE_1   ... FEATURE_2879 \n        instrument  datetime              \n        SH600000    2020-09-15  0.592733    1.037304    ... 0.592733      \n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" date "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" date_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            target "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get date"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("specific total volumne "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("order data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" colname "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" toArray"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("backtest"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            add queue"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ins"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" date"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" colname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feature"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" target"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_buy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])]),t._v(" "),s("Tab",{attrs:{label:"Logger"}},[s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train_logger "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" logger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("InfoLogger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nself"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_logger "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("getattr")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" io_conf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test_logger"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("GLR")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Calculate -P(value | value > 0) / P(value | value < 0)")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("DFLogger")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    save res in .pkl file and df in .log file\n\n    exampler df file (2607 rows × 5 columns)\n                                                $v_t        $max_vol_t  $traded_t   $vwap_t action\n    SH600000    2021-01-04 09:30:00 2021-01-04  1815.849349 215362624.0 1815.849349 0.972054    4.0\n                2021-01-04 09:31:00 2021-01-04  1815.849349 85798352.0  1815.849349 0.971212    4.0\n                ...\n                2021-01-18 14:56:00 2021-01-18  NaN 48203100.0  NaN 1.019192    NaN\n    \n    exampler result file\n                                  target    sell    vwap    this_vv_ratio   this_ffr\n    SH600000    2021-01-04  54475.480469    True    0.969405    0.995769    100.0\n                2021-01-05  45715.070312    True    0.968810    0.999111    100.0\n                ...\n                2021-01-18  81973.843750    True    1.005258    0.996215    100.0\n    """')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("_worker")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("log_dir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" order_dir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" queue"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" info "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"stop"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("v "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("   \n                get summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k_std"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k_mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PR_sell/buy"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ffr_sell/buy"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PA_sell/buy"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                get summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("weighted_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" weighted by model_sell"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("buy\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"obs0_PR"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ffr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PA"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                get summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("weighted_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" weighted by money\n            summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'GLR_sell/buy'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" GLR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("PA_sell"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("buy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            queue"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" res "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" info"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"df"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" info"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"res"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            save instrument res "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pkl "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" df "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" info"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" v\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InfoLogger")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("DFLogger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""logger without save function? used during training\n    """')]),t._v("\n")])])])])],1)],1),t._v(" "),s("h2",{attrs:{id:"environment"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#environment"}},[t._v("#")]),t._v(" Environment")]),t._v(" "),s("h2",{attrs:{id:"policy"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#policy"}},[t._v("#")]),t._v(" Policy")]),t._v(" "),s("h2",{attrs:{id:"network-optimizer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#network-optimizer"}},[t._v("#")]),t._v(" Network & Optimizer")]),t._v(" "),s("h2",{attrs:{id:"reference"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#reference"}},[t._v("#")]),t._v(" Reference")]),t._v(" "),s("p",[t._v("[1] Fang, Y., Ren, K., Liu, W., Zhou, D., Zhang, W., Bian, J., ... & Liu, T. Y. (2021). Universal Trading for Order Execution with Oracle Policy Distillation. arXiv preprint arXiv:2103.10860.")])])}),[],!1,null,null,null);a.default=e.exports}}]);