(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{1218:function(t,e,a){"use strict";a.r(e);var s=a(2),v=Object(s.a)({},(function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("div",{staticClass:"custom-block tip"},[s("p",[t._v("在这里记录LeNet, AlexNet, VGG, GoogleNet, ResNet, DenseNet, SENet和MobileNet异同")])]),t._v(" "),s("h2",{attrs:{id:"lenet"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#lenet"}},[t._v("#")]),t._v(" LeNet")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",[t._v("由LeCun于1998年提出, 用于解决手写数字识别任务. 组成元素包括卷积层、池化层、全连接层")]),t._v(" "),s("blockquote",[s("p",[s("img",{attrs:{src:a(623),alt:"LeNet"}})]),t._v(" "),s("ol",[s("li",[t._v("输入一个"),s("code",[t._v("28 x 28")]),t._v("的图像, 用"),s("code",[t._v("np.pad")]),t._v("填充至"),s("code",[t._v("32 x 32")])]),t._v(" "),s("li",[t._v("经过尺寸为"),s("code",[t._v("5 x 5")]),t._v(", 步长为"),s("code",[t._v("1")]),t._v(", 卷积核数目为"),s("code",[t._v("6")]),t._v("的卷积层后, 图像尺寸变为"),s("code",[t._v("32-5+1=28")]),t._v(", 即"),s("code",[t._v("28 x 28 x 6")])]),t._v(" "),s("li",[t._v("第一个池化核尺寸为"),s("code",[t._v("2 x 2")]),t._v("(batch, height, width, channel), 步长为"),s("code",[t._v("2")]),t._v("(batch, stride, stride, channel), 池化后图像尺寸减半, 变为"),s("code",[t._v("14 x 14 x 6")])]),t._v(" "),s("li",[t._v("第二个卷积核尺寸为"),s("code",[t._v("5 x 5")]),t._v(", 步长为"),s("code",[t._v("1")]),t._v(", 卷积核数目为"),s("code",[t._v("16")]),t._v(", 图像尺寸变为"),s("code",[t._v("14-5+1=10")]),t._v(", 即"),s("code",[t._v("28 x 28 x 16")])]),t._v(" "),s("li",[t._v("第二个池化核同前面那个池化层一样, 图像尺寸减半得到"),s("code",[t._v("5x5x16")])]),t._v(" "),s("li",[t._v("最后几个全连接层得到"),s("code",[t._v("25 x 16 -> 120 -> 84 -> 10")]),t._v(", 激活函数都是relu")])])]),t._v(" "),s("details",{staticClass:"custom-block details"},[s("summary",[t._v("代码")]),t._v(" "),s("p",[t._v("<<< docs/codes/LeNet.py")])])]),t._v(" "),s("h2",{attrs:{id:"alexnet"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#alexnet"}},[t._v("#")]),t._v(" AlexNet")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",[t._v("2012年，Alex Krizhevsky, Ilya Sutskever, Geoff Hinton等人设计出了AlexNet，夺得了2012年ImageNet LSVRC的冠军, 错误率15.3%远超过第二名的26.2%")]),t._v(" "),s("blockquote",[s("p",[s("img",{attrs:{src:a(624)+"#center",alt:"LeNet"}})]),t._v(" "),s("ol",[s("li",[t._v("包含5层卷积层和三层全连接层, 相比LeNet增加了深度, 使用了线性激活函数"),s("code",[t._v("ReLU")]),t._v(", 使用了"),s("code",[t._v("Dropout")]),t._v(","),s("code",[t._v("Data Augmentation")]),t._v("这些防止过拟合的办法")]),t._v(" "),s("li",[t._v("相比"),s("code",[t._v("sigmoid/tanh")]),t._v(", 由于"),s("code",[t._v("ReLU")]),t._v("是线性的, 导数始终是"),s("code",[t._v("1")]),t._v(", 计算量大大减少, 收敛速度会加快不少, 而且避免了梯度消失(sigmoid在很多地方导数接近"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mn",[t._v("0")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("0")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"0.64444em","vertical-align":"0em"}}),s("span",{staticClass:"mord"},[t._v("0")])])])]),t._v(")")]),t._v(" "),s("li",[t._v("数据扩充(data augmentation)指的是通过对原始数据(比如照片)进行一些翻转, 随机裁剪, 平移, 光照变化等, 改变输入数据, 但输出结果仍然不变, 相当于增加训练数据, 提高算法准确率")]),t._v(" "),s("li",[t._v("重叠池化(overlapping pooling), 传统池化是不重叠的, AlexNet要求池化窗口移动的步长小于窗口长度, 这样压缩没有以前厉害, 所以多加了几层?")]),t._v(" "),s("li",[t._v("局部归一(local response normalization), 因为"),s("code",[t._v("ReLU")]),t._v("的响应结果是无界的(无穷大), 通过抑制同一位置其他通道的特征值来抑制过拟合. 貌似就是让兴奋的更加兴奋, 如果周围所有神经元都很兴奋, 则归一化处理后数值上都不那么兴奋了")]),t._v(" "),s("li",[t._v("Dropout, 在深度学习网络的训练过程中, 按照一定的概率将一些神经元"),s("strong",[t._v("暂时")]),t._v("从网络中丢弃. 由于是随机丢弃, 故而每一个batch都在训练不同的网络, 相当于集成学习")])])]),t._v(" "),s("details",{staticClass:"custom-block details"},[s("summary",[t._v("代码")]),t._v(" "),s("p",[t._v("<<< docs/codes/AlexNet.py")])])]),t._v(" "),s("h2",{attrs:{id:"参考资料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("p",[t._v("[1] "),s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/176987177",target:"_blank",rel:"noopener noreferrer"}},[t._v("你应该知道的几种CNN网络与实现"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("[2] "),s("a",{attrs:{href:"https://easyfly007.github.io/blogs/index.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("卷积神经网络介绍"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("[3] "),s("a",{attrs:{href:"https://my.oschina.net/u/876354/blog/1633143",target:"_blank",rel:"noopener noreferrer"}},[t._v("大话CNN经典模型：AlexNet"),s("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=v.exports},623:function(t,e,a){t.exports=a.p+"assets/img/ml_cnn-1.bd427e5e.png"},624:function(t,e,a){t.exports=a.p+"assets/img/ml_cnn-2.6ab24962.png"}}]);