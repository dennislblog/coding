---
title: 第一阶段
date: 2021-01-27
autoGroup-1: 机器学习
tags:
  - Knowledge
---

::: tip
在这里记录机器学习笔记
:::

<!-- more -->

!!!include(./ml/book_01.md)!!!


## 决策树
::: right
分类速度快, 解释性好
:::

:::: tabs type: card
::: tab 特征选择
> 内部结点表示一个特征或属性, 叶结点表示一个类. 常用算法包括 1)信息增益(ID3), 信息增益比(C4.5)和基尼指数(CART)
> 
> - ID3: 选择信息增益(information gain, 得知特征X信息而使得Y分类的不确定性降低)大的特征. 对于数据集$\mathbb{D}$和特征集合$\mathcal{A}$, $\mathbb{D}_i$是当特征$\mathcal{A}$取第$i$个值的样本子集, $C_k$属于第$k$类样本子集
> > - 在平均分布下不确定性最大, 对有相同概率分布的不同的随机变量, 取值越多的随机变量熵越大(所以才有了C4.5)
> > - 其实和第一项$H(\mathbb{D})$关系不大, 就是选条件熵最小的特征
> $$\begin{aligned}
    & g(\mathbb{D},\mathcal{A}) = H(\mathbb{D}) - H(\mathbb{D} \vert \mathcal{A}) \\
    & H(\mathbb{D}) = -\sum_{k=1}^K \frac{|C_k|}{|\mathbb{D}|}\log_2 \frac{|C_k|}{|\mathbb{D}|} \qquad H(\mathbb{D} \vert \mathcal{A}) = \sum_{i=1}^n \frac{|\mathbb{D}_i|}{|\mathbb{D}|}H(\mathbb{D}_i)
\end{aligned}$$
>
> - C4.5： 选择信息增益比最大的特征, 解决了ID3总喜欢偏向取值较多的特征
> > - $H_{\mathcal{A}}(\mathbb{D})$的计算方式就是把特征的每一个取值看成一个分类, 然后描述特征不同取值在样本中的分布
> > - 纯度高的特征$H_{\mathcal{A}}(\mathbb{D})$就比较小, 更容易被选择
> $$g_R(\mathbb{D},\mathcal{A}) = \frac{g(\mathbb{D},\mathcal{A})}{H_{\mathcal{A}}(\mathbb{D})}$$
> 
> - CART： Classification and Regression Tree, 顾名思义, 可以进行分类和回归, 可以处理离散属性, 也可以处理连续的
> > - 基尼指数反映了从数据中随机抽取两个样本, 其类别标记不一致的概率, 计算比熵简单
> > - 基尼指数可以估计为熵的一阶泰勒展开
> $$\text{Gini}(\mathbb{D}, \mathcal{A}) = \sum_{i}^n\frac{|\mathbb{D}_i|}{|\mathbb{D}|}\text{Gini}(\mathbb{D}_i), \quad \text{Gini}(\mathbb{D}) = 1 - \sum_{k=1}^K \left(\frac{|C_k|}{|\mathbb{D}|}\right)^2$$
:::
::: tab 面试
参考[这里](https://zhuanlan.zhihu.com/p/85731206)和[这里](https://blog.csdn.net/zjsghww/article/details/51638126)

> - 决策树出现过拟合的原因及其解决办法？
> > - 剪枝(限制决策树的生长), 包括预剪枝(提前结束)和后剪枝(建立完决策树再修), 剪枝范围包括限制树的高度, 叶子节点数目, 最大叶子节点数, 特征不纯度等
> > - 样本噪音剔除
> > - 特征工程, 剔除与预测变量相关性较低的
> 
> - 决策树如何处理缺失值
> 
> - 决策树和逻辑回归的区别
> > - 决策树一旦某个节点确立, 不同节点之间的关系就被切断了, 以后各自切分各自的, 所以无法支持对多变量的同时检验. 而逻辑回归始终着眼整个数据的拟合
> > - 逻辑回归应用的是样本数据线性可分的场景，输出结果是概率; 线性回归应用的是样本数据和输出结果之间存在线性关系的场景, 自变量和因变量之间存在线性关系
> > - 逻辑回归对极值比较敏感，容易受极端值的影响，而决策树在这方面表现较好
:::
::::


## 逻辑回归和最大熵模型

::: right
解释性好, 样本数据线性可分
:::

:::: tabs type: card
::: tab 逻辑回归
> logistics分布
> $$f(x) = \frac{\exp(-(x-\mu)/\gamma)}{\gamma(1+\exp(-(x-\mu)/\gamma))^2}, \quad F(x) = \frac{1}{1+\exp(-(x-\mu)/\gamma)}$$
> > 后者是一条sigmoid曲线, 以$(\mu, \frac{1}{2})$为中心对称, 参数$\gamma$越小, 曲线在中心附近导数越大

> 逻辑回归是一种分类模型, 输出条件概率$P(Y | X), Y\in\{0,1\}, X\in \mathbb{R}$
> $$P(Y=0|x) = \frac{1}{1+\exp(w x + b)}, \quad P(Y=1|x) = 1 - P(Y=0|x)$$
> > 事件发生的log odds表示为$\log p/(1-p)$
> > 对于逻辑回归来说, 这个对数几率是一个线性函数： 
> > $$\log [P(Y=1|x)/1-P(Y=1|x)] = w\cdot x + b$$

> 参数估计, 假设$N$个数据, 设$P(Y=1|x) = \pi(x)$, 最大似然估计是
> $$\begin{aligned}
\mathcal{L}(\omega,b) &= \sum_{i=1}^N \left[ y_i\log\pi(x_i) + (1-y_i)\log(1-\pi(x_i)) \right] \\
    &=  \sum_{i=1}^N \left[ y_i\log\frac{\pi(x_i)}{1-\pi(x_i)} + \log(1-\pi(x_i)) \right] \\
    &= \sum_{i=1}^N [y_i(\omega x_i + b) - \log(1+\exp(\omega x_i + b))]
\end{aligned}$$
> 然后对$\omega, b$求导即可, 可以求得$\partial_{\omega_i} \mathcal{L} = (y_i - \pi(x_i))x_i$
:::
::: tab 最大熵模型
> 假设随机变量$Y \in \{A,B,C,D,E\}$, 已知$P(A) + P(B) = \frac{3}{10}$, 求最大熵模型(用$y_i$表示随机变量)
> $$\begin{aligned}
    \min \quad & -H(P) = \sum_{i=1}^5 P(y_i)\log P(y_i) \\
    s.t. \quad & P(y_1) + P(y_2) = \frac{3}{10} \\
               & \sum_{i=1}^5 P(y_i) = 1
\end{aligned}$$
> 引入拉格朗日乘子$\lambda_1, \lambda_2$
> $$\mathcal{L}(P, \lambda) = \sum_{i=1}^5 P(y_i)\log P(y_i) + \lambda_1\left(P(y_1) + P(y_2) - \frac{3}{10}\right) + \lambda_2 \left(\sum_{i=1}^5P(y_i) - 1\right)$$
> 对$P(y_i) \forall i \in \{1,2,3,4,5\}$求导, 得到
> $$\begin{aligned}
    & P(y_1) = P(y_2) = \exp(-\lambda_1-\lambda_2-1) \\
    & P(y_3) = P(y_4) = P(y_5) = \exp(-\lambda_2-1) 
\end{aligned}$$
> 最后再对$\lambda_1, \lambda_2$求偏导, 得到最终基于最大熵的概率分布为$\left\{\frac{3}{20}, \frac{3}{20}, \frac{7}{30}, \frac{7}{30}, \frac{7}{30}\right\}$

:::
::: tab 面试
> - 如何防止过拟合
> > ![正则化](~@assets/ml_01_09.png#right) 
> > - 在目标函数或代价函数后面加上一个正则项, 限制参数选择
> > - 正则项相当于给参数加一个先验估计, $l_1$是拉普拉斯先验, 而$l_2$正则则是高斯先验. $l_1$产生稀疏解, 让一部分特征的系数缩小到$0$, 从而间接实现特征选择. 
> > - 见右图, $l_1$基本上会相切于顶点处, 即舍弃了一部分特征(所以更适用于特征之间关联不大的情况)

> - 为何逻辑回归结果比线性回归要好
> > - 线性回归用最小二乘法优化目标函数, 而逻辑回归基于最大似然估计参数; 为啥不同呢? 因为各自任务的$y$服从不同的概率分布, 在线性回归中假设$y$服从正态分布, 而逻辑回归中假设$y$服从伯努利分布
> > - 逻辑回归是一种将预测值限定在$[0,1]$之间的回归模型, 逻辑回归是最大熵对应类别为$2$时的特殊情况, 当将分类对象扩充为多类时, 逻辑回归就是最大熵模型
:::
::::

<!-- 判别模型不关心数据怎么生成的, 只关心数据之间的差别, 然后用差别来对给定数据进行分类； 参数模型的前提是假设数据服从某一分布，该分布由一些参数确定-->

## 支持向量机

::: right
用和分类相关的少数点去学习分类器, 基于HingeLoss的非参数模型
:::

:::: tabs type: card
::: tab 基础
> 假设数据线性可分, 利用间隔最大化原理优化超平面, 具有唯一解
> $$\begin{aligned}
& \text{函数间隔: } \quad \gamma_i = y_i(\omega x_i + b)
& \text{几何间隔: } \quad \gamma_i = y_i\left(\frac{\omega}{\|\omega\|_2} x_i + \frac{b}{\|\omega\|_2}\right)
\end{aligned}$$
> - 如果$\|\omega\|_2=1$, 则函数间隔等于几何间隔
> - 如果超平面参数$\omega, b$成比例地改变, 那么函数间隔也成比例改变, 但几何间隔不变化

> 间隔最大化思想
> ![向量机](~@assets/ml_01_10.png#right) 
> $n$维空间$x \in \mathbb{R}^n$到超平面$\omega^T x + b =0$的距离是
> $$\frac{|\omega^T x + b|}{\|\omega\|}, \; \|\omega\| = \sqrt{\omega_1^2 + \cdots, \omega_n^2}$$
> 现在我们把分子的绝对值符号拿掉, 设其为$d(x)$, 我们希望对于$y=1|x$的数据有$d(x) \geq d$, 对于$y=-1|x$的数据有$d(x) \leq -d$, 这一部分可以简写为
> $$\begin{rcases}
   \frac{\omega^Tx + b}{\|\omega\|d}\geq 1 &\text{if } y=1 \\
   \frac{\omega^Tx + b}{\|\omega\|d}\leq 1 &\text{if } y=-1
\end{rcases}⇒ y(\omega^Tx + b) \geq 1$$
> 我们把$y_i(\omega x_i + b) = 1$的数据标定为支持向量, 每个$(x_i, y_i)$到超平面的距离可以写为
> $$d = \frac{|\omega^Tx + b|}{\|\omega\|} = \frac{y(\omega^Tx + b)}{\|\omega\|}, \quad y(\omega^Tx + b) = 1$$
> 因此我们的目标就是最大化这一超平面间隔, $\max \frac{2}{\|\omega\|} \Rightarrow \min \frac{1}{2}\|\omega\|$, 为了计算方便将$l_2$范数平方, 因此我们得到了SVM的优化方程
> $$\begin{aligned}
    \min \quad & \frac{1}{2}\|\omega\|^2 \\
    s.t. \quad & y_i(\omega x_i + b) - 1 \geq 0, \; i = 1, \cdots, N
\end{aligned}$$
> - 都分对几乎不可能(限制条件), 这不就是一个带$l_2$的逻辑回归问题?
> - 决定分离超平面时只有少部分`支持向量机`起到作用, 而其他实例被无视了
:::
::: tab 对偶问题
> 对原优化目标进行拉格朗日求解, 得到$\sum \lambda_ix_iy_i = \omega, \sum \lambda_iy_i = 0$, 代入到原方程, 我们得到
> $$\begin{aligned}
    \min \quad & \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \lambda_i\lambda_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\lambda_i
    s.t. \quad & \sum_{i=1}^N \lambda_iy_i = 0, \; i = 1, \cdots, N, \; 0\leq \lambda_i \leq C
\end{aligned}$$
> - 对于这个二次规划问题, 用SMO(Sequential Minimal Optimization)算法求解, 根据拉格朗日的定义, $\lambda_i > 0$对应的$(x_i,y_i)$就是支持向量$(\omega x_i + b = 1)$
> - 求解得到$f(x) = \text{sign}\left(\sum_{i=1}^N \lambda_i y_i (x \cdot x_i) + b\right)$

> 软间隔： 将SVM扩展到线性不可分, 引入松弛变量$\xi_i, C > 0$, 松弛越多惩罚越大
> ![软间隔](~@assets/ml_01_11.png#right) 
> $$\begin{aligned}
    \min \quad & \frac{1}{2}\|\omega\|^2 + C\sum_{i=1}^N\xi_i \\
    s.t. \quad & y_i(\omega x_i + b) \geq 1-\xi_i, \; i = 1, \cdots, N \\
    & \xi_i \geq 0 \; i = 1, \cdots, N 
\end{aligned}$$
> - 这个$\xi_i$只有可能取$2$或者$0$吧? 通过$C$来控制我们对错误的容忍, 当$C\nearrow$, 趋向于硬间隔

> 线性不可分 -> 核函数, 用$\phi(x)$表示将原始特征映射到新向量的值
:::
::::

<!-- 

 -->